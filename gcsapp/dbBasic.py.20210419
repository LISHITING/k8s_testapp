#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

"""
A simple example demonstrating basic Spark SQL features.
Run with:
  ./bin/spark-submit examples/src/main/python/sql/basic.py
"""
from os.path import abspath
import subprocess
# $example on:init_session$
from pyspark.sql import SparkSession
# $example off:init_session$

# $example on:schema_inferring$
from pyspark.sql import Row
# $example off:schema_inferring$

# $example on:programmatic_schema$
# Import data types
from pyspark.sql.types import StringType, StructType, StructField
# $example off:programmatic_schema$


def basic_df_example(spark):
    # $example on:create_df$
    # spark is an existing SparkSession
    print("===============================Read from GCS  testing=======")

    df = spark.read.json("gs://ravinuodb-bucket/input/people.json")
    # Displays the content of the DataFrame to stdout
    df.show()
    # +----+-------+
    # | age|   name|
    # +----+-------+
    # |null|Michael|
    # |  30|   Andy|
    # |  19| Justin|
    # +----+-------+
    # $example off:create_df$

    # $example on:untyped_ops$
    # spark, df are from the previous example
    # Print the schema in a tree format
    df.printSchema()
    # root
    # |-- age: long (nullable = true)
    # |-- name: string (nullable = true)

    # Select only the "name" column
    df.select("name").show()
    # +-------+
    # |   name|
    # +-------+
    # |Michael|
    # |   Andy|
    # | Justin|
    # +-------+

    # Select everybody, but increment the age by 1
    df.select(df['name'], df['age'] + 1).show()
    # +-------+---------+
    # |   name|(age + 1)|
    # +-------+---------+
    # |Michael|     null|
    # |   Andy|       31|
    # | Justin|       20|
    # +-------+---------+

    # Select people older than 21
    df.filter(df['age'] > 21).show()
    # +---+----+
    # |age|name|
    # +---+----+
    # | 30|Andy|
    # +---+----+

    # Count people by age
    df.groupBy("age").count().show()
    # +----+-----+
    # | age|count|
    # +----+-----+
    # |  19|    1|
    # |null|    1|
    # |  30|    1|
    # +----+-----+
    # $example off:untyped_ops$

    # $example on:run_sql$
    # Register the DataFrame as a SQL temporary view
    df.createOrReplaceTempView("people")

    sqlDF = spark.sql("SELECT * FROM people")
    sqlDF.show()
    # +----+-------+
    # | age|   name|
    # +----+-------+
    # |null|Michael|
    # |  30|   Andy|
    # |  19| Justin|
    # +----+-------+
    # $example off:run_sql$

    # $example on:global_temp_view$
    # Register the DataFrame as a global temporary view
    df.createGlobalTempView("people")

    # Global temporary view is tied to a system preserved database `global_temp`
    spark.sql("SELECT * FROM global_temp.people").show()
    # +----+-------+
    # | age|   name|
    # +----+-------+
    # |null|Michael|
    # |  30|   Andy|
    # |  19| Justin|
    # +----+-------+

    # Global temporary view is cross-session
    spark.newSession().sql("SELECT * FROM global_temp.people").show()
    # +----+-------+
    # | age|   name|
    # +----+-------+
    # |null|Michael|
    # |  30|   Andy|
    # |  19| Justin|
    # +----+-------+
    # $example off:global_temp_view$

def loadKV(spark):
    print("===============================Create DB Read from GCS  testing=======")
    spark.sql("create database IF NOT EXISTS mikeDB LOCATION 'gs://ravinuodb-bucket/temp'")
    spark.sql("DESCRIBE DATABASE EXTENDED mikeDB;")
 
    spark.sql("use mikeDB")
 
    print("===============================Create DBtable  Read from GCS  testing=======")
    # spark is an existing SparkSession
    spark.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive")

    print("===============================Create DBtable end  Read from GCS  testing=======")
    spark.sql("LOAD DATA INPATH 'gs://ravinuodb-bucket/input/kv1.txt' INTO TABLE src")

    print("===============================Create DBtable endshpow  Read from GCS  testing=======")
    # Queries are expressed in HiveQL
    spark.sql("SELECT * FROM src").show()
    # +---+-------+
    # |key|  value|
    # +---+-------+
    # |238|val_238|
    # | 86| val_86|
    # |311|val_311|
    # ...

    # Aggregation queries are also supported.
    spark.sql("SELECT COUNT(*) FROM src").show()
    # +--------+
    # |count(1)|
    # +--------+
    # |    500 |
    # +--------+

    # The results of SQL queries are themselves DataFrames and support all normal functions.
    sqlDF = spark.sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")

    # The items in DataFrames are of type Row, which allows you to access each column by ordinal.
    stringsDS = sqlDF.rdd.map(lambda row: "Key: %d, Value: %s" % (row.key, row.value))
    for record in stringsDS.collect():
        print(record)
    # Key: 0, Value: val_0
    # Key: 0, Value: val_0
    # Key: 0, Value: val_0
    # ...

    # You can also use DataFrames to create temporary views within a SparkSession.
    Record = Row("key", "value")
    recordsDF = spark.createDataFrame([Record(i, "val_" + str(i)) for i in range(1, 101)])
    recordsDF.createOrReplaceTempView("records")

    # Queries can then join DataFrame data with data stored in Hive.
    spark.sql("SELECT * FROM records r JOIN src s ON r.key = s.key").show()
    # +---+------+---+------+
    # |key| value|key| value|
    # +---+------+---+------+
    # |  2| val_2|  2| val_2|
    # |  4| val_4|  4| val_4|
    # |  5| val_5|  5| val_5|
    # ...
    # $example off:spark_hive$
    print("===============================Output DF  of joining tables =======")
    sqlDF = spark.sql("SELECT * FROM src, global_temp.people").show()
    output = spark.sql("SELECT * FROM src, global_temp.people")

    print("===============================Output DF  to GCS testing=======")
    output.write.format("csv").save("gs://ravinuodb-bucket/output/csv/")
    # df.write.parquet("gs://ravinuodb-bucket/output/mikeoutput/")

def schema_inference_example(spark):
    # $example on:schema_inferring$
    sc = spark.sparkContext

    # Load a text file and convert each line to a Row.
    lines = sc.textFile("gs://ravinuodb-bucket/input/people.txt")
    parts = lines.map(lambda l: l.split(","))
    people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))

    # Infer the schema, and register the DataFrame as a table.
    schemaPeople = spark.createDataFrame(people)
    schemaPeople.createOrReplaceTempView("people")

    # SQL can be run over DataFrames that have been registered as a table.
    teenagers = spark.sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")

    # The results of SQL queries are Dataframe objects.
    # rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.
    teenNames = teenagers.rdd.map(lambda p: "Name: " + p.name).collect()
    for name in teenNames:
        print(name)
    # Name: Justin
    # $example off:schema_inferring$
    print("===============================End of metthod:schema_inference_example  =======")


def programmatic_schema_example(spark):
    # $example on:programmatic_schema$
    sc = spark.sparkContext

    # Load a text file and convert each line to a Row.
    lines = sc.textFile("gs://ravinuodb-bucket/input/people.txt")
    parts = lines.map(lambda l: l.split(","))
    # Each line is converted to a tuple.
    people = parts.map(lambda p: (p[0], p[1].strip()))

    # The schema is encoded in a string.
    schemaString = "name age"

    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]
    schema = StructType(fields)

    # Apply the schema to the RDD.
    schemaPeople = spark.createDataFrame(people, schema)

    # Creates a temporary view using the DataFrame
    schemaPeople.createOrReplaceTempView("people")

    # SQL can be run over DataFrames that have been registered as a table.
    results = spark.sql("SELECT name FROM people")

    results.show()
    # +-------+
    # |   name|
    # +-------+
    # |Michael|
    # |   Andy|
    # | Justin|
    # +-------+
    # $example off:programmatic_schema$
    print("===============================End of metthod:programmatic_schema_example =======")

if __name__ == "__main__":
    # $example on:init_session$
    warehouse_location = abspath('spark-warehouse')
    print("====================== warehouse_location PATH:" + warehouse_location)
    spark = SparkSession \
        .builder \
        .appName("Python Spark SQL basic example") \
        .config("spark.some.config.option", "some-value") \
        .config("spark.sql.warehouse.dir", "gs://ravinuodb-bucket/input/warehouse_location") \
        .enableHiveSupport() \
        .getOrCreate()
    # $example off:init_session$

    print("===============================Create DB Read from GCS  testing=======")
    spark.sql("create database IF NOT EXISTS mikeDB LOCATION 'gs://ravinuodb-bucket/temp'")
    spark.sql("DESCRIBE DATABASE EXTENDED mikeDB;")
    print("============================== spark.catalog.listDatabases.show  testing=======")
    spark.catalog.listDatabases()

    print("=============================")
 
    subprocess.Popen("PWD ",stdout=subprocess.PIPE,stderr=subprocess.STDOUT, shell=True)
    print("=====Popen  . ========================")
    result= subprocess.Popen("ls -ltr .",stdout=subprocess.PIPE,stderr=subprocess.STDOUT, shell=True)
    print(result.stdout.read())
    print("=============================")
    print("=============================")
    cmd = "ls -lra /opt"
    result= subprocess.Popen("ls -ltr /home/",stdout=subprocess.PIPE,stderr=subprocess.STDOUT, shell=True)
    print(result.stdout.read())
    print("=============================")
    spark.sql("use mikeDB")
 
    print("===============================Create DBtable  Read from GCS  testing=======")
    # spark is an existing SparkSession
    spark.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive")
    print("=============================")
    spark.sql('show tables from default').show()

    print("===============================Create DBtable end  Read from GCS  testing=======")
    #spark.sql("LOAD DATA INPATH 'gs://ravinuodb-bucket/input/kv1.txt' OVERWRITE  INTO TABLE src")
    print("=============================")
    spark.sql('show tables from default').show()

    print("=============================")
    spark.sql("SELECT * FROM src").show()
    print("===============================Query DONE")
    sqlDF = spark.sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")
    print("===============================Output DF  to GCS testing=======")
    #sqlDF.write.format("csv").save("gs://ravinuodb-bucket/output/csv/")
    print("===============================Output DF  to GCS  DONE  testing=======")

    cmd = 'pwd'
    result = subprocess.call(cmd)
    print("=============================")


    result= subprocess.Popen("gsutil cp ./metastore_db gs://ravinuodb-bucket/output/",stdout=subprocess.PIPE,stderr=subprocess.STDOUT, shell=True)
    print(result.stdout.read())
    print("=============================")
    print(result.stderr.read())
    print("=============================")
    print("stdout:", result.stdout)
    print("stderr:", result.stderr)
    #basic_df_example(spark)
    #loadKV(spark)
    # schema_inference_example(spark)
    programmatic_schema_example(spark)
    
    spark.stop()


